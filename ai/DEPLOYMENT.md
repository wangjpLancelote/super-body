# AI Module Deployment Guide

This guide explains how to deploy the AI module for the SuperBody application.

## Prerequisites

1. Node.js 18+ installed
2. Supabase project created
3. API keys for OpenAI or Anthropic
4. Supabase CLI for local development

## Local Development

### 1. Install Dependencies

```bash
cd ai
npm install
```

### 2. Set Up Environment Variables

Create a `.env.local` file:

```env
# OpenAI Configuration (optional)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_ORG_ID=your_openai_org_id_here

# Anthropic Configuration (optional)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Supabase Configuration
SUPABASE_URL=your_supabase_url
SUPABASE_ANON_KEY=your_supabase_anon_key
SUPABASE_SERVICE_ROLE_KEY=your_supabase_service_role_key

# AI Assistant Configuration
AI_ASSISTANT_DRY_RUN=true
AI_ASSISTANT_MODEL=gpt-3.5-turbo
AI_ASSISTANT_MAX_TOKENS=2000
AI_ASSISTANT_TEMPERATURE=0.7
```

### 3. Run Tests

```bash
npm test
```

### 4. Build the Module

```bash
npm run build
```

## Supabase Deployment

### 1. Deploy Edge Function

```bash
# Deploy the AI assistant function
supabase functions deploy ai-assistant

# Or deploy all functions
supabase functions deploy
```

### 2. Set Up Environment Variables in Supabase

```bash
# Set OpenAI API key
supabase secrets set OPENAI_API_KEY=your_openai_api_key

# Set Anthropic API key
supabase secrets set ANTHROPIC_API_KEY=your_anthropic_api_key

# Set Supabase configuration
supabase secrets set SUPABASE_URL=your_supabase_url
supabase secrets set SUPABASE_ANON_KEY=your_supabase_anon_key
supabase secrets set SUPABASE_SERVICE_ROLE_KEY=your_supabase_service_role_key

# Set AI assistant configuration
supabase secrets set AI_ASSISTANT_DRY_RUN=true
supabase secrets set AI_ASSISTANT_MODEL=gpt-3.5-turbo
supabase secrets set AI_ASSISTANT_MAX_TOKENS=2000
supabase secrets set AI_ASSISTANT_TEMPERATURE=0.7
```

### 3. Configure CORS

Ensure the Edge Function has proper CORS headers:

```typescript
// In your Edge Function
const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers':
    'authorization, x-client-info, apikey, content-type',
  'Access-Control-Allow-Methods': 'GET, POST, PATCH, OPTIONS',
};
```

### 4. Set Up Database Schema

Ensure you have the necessary tables:

```sql
-- AI logs table
create table ai_logs (
  id bigint generated by default as identity primary key,
  user_id uuid references auth.users(id) not null,
  query text not null,
  action text not null,
  result jsonb,
  metadata jsonb,
  is_dry_run boolean default true,
  created_at timestamp with time zone default timezone('utc'::text, now()) not null
);

-- Document embeddings table (for semantic search)
create table document_embeddings (
  id bigint generated by default as identity primary key,
  user_id uuid references auth.users(id) not null,
  document_id text not null,
  title text,
  content text,
  embedding vector(1536) not null,
  created_at timestamp with time zone default timezone('utc'::text, now()) not null
);

-- Create vector extension if not exists
create extension if not exists vector;
```

## Production Deployment

### 1. Configure Production Settings

Update your production environment variables:

```env
# Production configuration
AI_ASSISTANT_DRY_RUN=false  # Disable dry-run in production
AI_ASSISTANT_MODEL=gpt-4    # Use more powerful model
AI_ASSISTANT_MAX_TOKENS=4000
AI_ASSISTANT_TEMPERATURE=0.3  # More deterministic responses
```

### 2. Rate Limiting

Implement rate limiting in the Edge Function:

```typescript
// Add rate limiting middleware
const rateLimit = new Map<string, { count: number; resetTime: number }>();

function checkRateLimit(userId: string): boolean {
  const now = Date.now();
  const userLimit = rateLimit.get(userId);

  if (!userLimit || now > userLimit.resetTime) {
    rateLimit.set(userId, { count: 1, resetTime: now + 60000 }); // 1 minute window
    return true;
  }

  if (userLimit.count >= 10) { // 10 requests per minute
    return false;
  }

  userLimit.count++;
  return true;
}
```

### 3. Error Handling

Implement comprehensive error handling:

```typescript
// Global error handler
function handleError(error: unknown): Response {
  console.error('AI Assistant Error:', error);

  if (error instanceof Error) {
    return jsonResponse({
      error: error.message,
      timestamp: new Date().toISOString(),
    }, 500);
  }

  return jsonResponse({
    error: 'Internal server error',
    timestamp: new Date().toISOString(),
  }, 500);
}
```

### 4. Monitoring and Logging

Set up monitoring for the AI service:

```typescript
// Log all requests
await supabase.from('ai_logs').insert({
  user_id: user.id,
  query: body.input,
  action: action,
  metadata: {
    timestamp: new Date().toISOString(),
    userAgent: req.headers.get('user-agent'),
    ip: req.headers.get('x-forwarded-for'),
  },
  is_dry_run: body.dry_run,
});
```

## Web App Deployment

### 1. Update Environment Variables

Add these to your web app's `.env.local`:

```env
NEXT_PUBLIC_SUPABASE_URL=your_supabase_url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_supabase_anon_key
NEXT_PUBLIC_SUPABASE_EDGE_URL=your_supabase_url/functions/v1
```

### 2. Enable Streaming

Use the streaming component in your web app:

```tsx
import { AIAssistantStreaming } from '@/ai/components/AIAssistantStreaming';

function MyAIComponent() {
  return (
    <AIAssistantStreaming
      enableStreaming={true}
      dryRun={process.env.NODE_ENV === 'development'}
    />
  );
}
```

### 3. Handle API Keys Securely

Store API keys in environment variables, never in client-side code:

```typescript
// Only use in server-side code
const apiKey = process.env.OPENAI_API_KEY;

// Never expose to client
// const clientApiKey = process.env.NEXT_PUBLIC_OPENAI_API_KEY; // ‚ùå
```

## Performance Optimization

### 1. Caching

Implement caching for frequent requests:

```typescript
const cache = new Map<string, { data: any; expires: number }>();

async function getCachedResponse(key: string, ttl: number = 300000): Promise<any> {
  const cached = cache.get(key);
  if (cached && Date.now() < cached.expires) {
    return cached.data;
  }

  const data = await expensiveOperation();
  cache.set(key, { data, expires: Date.now() + ttl });
  return data;
}
```

### 2. Batch Processing

Batch multiple tool executions:

```typescript
async function batchExecuteTools(tools: Array<{ name: string; args: any }>) {
  const results = await Promise.allSettled(
    tools.map(tool => executeTool(tool.name, tool.args))
  );

  return results.map((result, index) => ({
    tool: tools[index].name,
    success: result.status === 'fulfilled',
    data: result.status === 'fulfilled' ? result.value : result.reason,
  }));
}
```

### 3. Connection Pooling

Use connection pooling for database operations:

```typescript
import { createPool } from 'pg';

const pool = createPool({
  connectionString: process.env.DATABASE_URL,
  max: 10,
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
});
```

## Security Considerations

### 1. Input Validation

Validate all user inputs:

```typescript
import { z } from 'zod';

const userInputSchema = z.object({
  message: z.string().min(1).max(1000),
  context: z.object({
    userId: z.string().uuid(),
    chatHistory: z.array(z.any()).max(10),
  }).optional(),
});

function validateInput(input: any) {
  return userInputSchema.parse(input);
}
```

### 2. Rate Limiting

Implement rate limiting at multiple levels:

- Function level (as shown above)
- User level
- API level

### 3. Content Filtering

Implement content filtering for user inputs:

```typescript
function containsMaliciousContent(input: string): boolean {
  const maliciousPatterns = [
    /password/i,
    /credit card/i,
    /social security/i,
    /confidential/i,
  ];

  return maliciousPatterns.some(pattern => pattern.test(input));
}
```

## Troubleshooting

### Common Issues

1. **API Key Not Found**
   - Check environment variables are set
   - Verify the correct key is being used

2. **CORS Errors**
   - Check CORS headers in Edge Function
   - Verify client requests include proper headers

3. **Rate Limit Errors**
   - Implement proper rate limiting
   - Use exponential backoff for retries

4. **Memory Issues**
   - Monitor memory usage in Edge Function
   - Implement proper cleanup

### Debug Mode

Enable debug mode for troubleshooting:

```typescript
const DEBUG = process.env.DEBUG === 'true';

function debug(message: string, data?: any) {
  if (DEBUG) {
    console.log(`[DEBUG] ${message}`, data);
  }
}
```

## Support

For support, please check:

1. The [README.md](./README.md) for usage instructions
2. The [troubleshooting section](#troubleshooting) above
3. Open an issue in the repository

## Monitoring

Set up monitoring for:

- Request latency
- Error rates
- Token usage
- API quota usage
- User engagement metrics

Use tools like:

- Supabase Analytics
- Vercel Analytics
- Custom dashboards with Grafana